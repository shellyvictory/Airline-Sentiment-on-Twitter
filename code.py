# -*- coding: utf-8 -*-
"""Shelly Victory_Proyek Kesatu Membuat Model NLP dengan Tf

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S6RBPG8dMvv7KtOWmOWZNemMLRFPU_Qm
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import os
os.listdir('sample_data')
import pandas as pd

Tweets = pd.read_csv('sample_data/Tweets.csv')
Tweets.tail() #melihat detail 5 data terakhir

# menjalankan proses one-hot-encoding dan membuat df baru
del Tweets['tweet_id']
del Tweets['retweet_count']
del Tweets['tweet_created']
del Tweets['tweet_location']
Tweets.rename(columns={'airline_sentiment': 'sentiment'}, inplace=True) #mengubah nama kolom df
category = pd.get_dummies(Tweets.sentiment)
df_new = pd.concat([Tweets, category], axis=1)
df_new = df_new.drop(columns='sentiment')
df_new

# mengubah nilai dari df kedalam tipe data np
text = df_new['text'].values
label = df_new[['negative', 'neutral', 'positive']].values

# membagi dataset
text_training, text_test, label_training, label_test = train_test_split(text, label, test_size=0.2)

#tokenisasi agar teks dipahami model
tokenizer = Tokenizer(num_words=5000, oov_token='x')
tokenizer.fit_on_texts(text_training)
tokenizer.fit_on_texts(text_test)

sequence_training = tokenizer.texts_to_sequences(text_training)
sequence_test = tokenizer.texts_to_sequences(text_test)

padded_training = pad_sequences(sequence_training)
padded_test = pad_sequences(sequence_test)

model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(input_dim=5000, output_dim=16),
                             tf.keras.layers.LSTM(64),
                             tf.keras.layers.Dense(128, activation='relu'),
                             tf.keras.layers.Dense(64, activation='relu'),
                             tf.keras.layers.Dense(3, activation='softmax')
])

model.compile(loss='categorical_crossentropy',
              optimizer='Adam',
              metrics=['accuracy'])

# melatih model
num_epochs = 40
history = model.fit(padded_training, label_training, epochs=num_epochs,
                    validation_data=(padded_test, label_test), verbose=2)